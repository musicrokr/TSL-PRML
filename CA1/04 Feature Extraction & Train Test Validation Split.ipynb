{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction & Train Test Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words and Sequence Vectors Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df = \"./Pickles/all_articles_processed.pickle\"\n",
    "\n",
    "with open(path_df, 'rb') as data:\n",
    "    articles = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>category</th>\n",
       "      <th>length_characters</th>\n",
       "      <th>length_words</th>\n",
       "      <th>category_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>The Straits Times</td>\n",
       "      <td>Will Chinese teen icon Wang Yuan's career go u...</td>\n",
       "      <td>wang yuans reputation career danger go smoke ...</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>1697</td>\n",
       "      <td>292</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>The Straits Times</td>\n",
       "      <td>The most value-for-money CBD set lunches</td>\n",
       "      <td>singapore  weekday lunch hour central busines...</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>8538</td>\n",
       "      <td>1363</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2904</th>\n",
       "      <td>AsiaOne</td>\n",
       "      <td>How much does adopting a child in Singapore co...</td>\n",
       "      <td>moment melissa hold weekold baby girl arm coul...</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>8825</td>\n",
       "      <td>1508</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5707</th>\n",
       "      <td>Channel News Asia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>singapore find favourite carrot cake roti prat...</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>3351</td>\n",
       "      <td>546</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>The Straits Times</td>\n",
       "      <td>Fans fishing for answers to question of whethe...</td>\n",
       "      <td>water murky whether malaysian singer fish leo...</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>1409</td>\n",
       "      <td>245</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 source                                              title  \\\n",
       "749   The Straits Times  Will Chinese teen icon Wang Yuan's career go u...   \n",
       "35    The Straits Times           The most value-for-money CBD set lunches   \n",
       "2904            AsiaOne  How much does adopting a child in Singapore co...   \n",
       "5707  Channel News Asia                                                NaN   \n",
       "195   The Straits Times  Fans fishing for answers to question of whethe...   \n",
       "\n",
       "                                                article   category  \\\n",
       "749    wang yuans reputation career danger go smoke ...  Lifestyle   \n",
       "35     singapore  weekday lunch hour central busines...  Lifestyle   \n",
       "2904  moment melissa hold weekold baby girl arm coul...  Lifestyle   \n",
       "5707  singapore find favourite carrot cake roti prat...  Lifestyle   \n",
       "195    water murky whether malaysian singer fish leo...  Lifestyle   \n",
       "\n",
       "      length_characters  length_words  category_code  \n",
       "749                1697           292              3  \n",
       "35                 8538          1363              3  \n",
       "2904               8825          1508              3  \n",
       "5707               3351           546              3  \n",
       "195                1409           245              3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(articles['article'], articles['category_code'], test_size=0.2, random_state=1)\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles in Training Dataset: 2400 80.0 %\n",
      "Articles in Testing Dataset: 300 10.0 %\n",
      "Articles in Validation Dataset: 300 10.0 %\n",
      "Total Number of Articles: 3000\n"
     ]
    }
   ],
   "source": [
    "all_article_count = len(X_train)+len(X_test)+len(X_validation)\n",
    "print('Articles in Training Dataset:',len(X_train),round(len(X_train)/all_article_count,2)*100,\"%\")\n",
    "print('Articles in Testing Dataset:',len(X_test),round(len(X_test)/all_article_count,2)*100,\"%\")\n",
    "print('Articles in Validation Dataset:',len(X_validation),round(len(X_validation)/all_article_count,2)*100,\"%\")\n",
    "print('Total Number of Articles:',all_article_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf Encoding (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 5000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "# Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "kwargs = {\n",
    "        'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "        'dtype': 'int32',\n",
    "        'strip_accents': 'unicode',\n",
    "        'decode_error': 'replace',\n",
    "        'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "        'max_features': TOP_K,\n",
    "        'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1616: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 5000)\n",
      "(300, 5000)\n",
      "(300, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Learn vocabulary from training texts and vectorize training texts.\n",
    "tdidf_features_train = vectorizer.fit_transform(X_train).toarray()\n",
    "print(tdidf_features_train.shape)\n",
    "\n",
    "# Vectorize testing texts.\n",
    "tdidf_features_test = vectorizer.transform(X_test).toarray()\n",
    "print(tdidf_features_test.shape)\n",
    "\n",
    "\n",
    "# Vectorize validation texts.\n",
    "tdidf_features_validation = vectorizer.transform(X_validation).toarray()\n",
    "print(tdidf_features_validation.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select top 'k' of the vectorized features.\n",
    "selector = SelectKBest(f_classif, k=min(TOP_K, tdidf_features_train.shape[1]))\n",
    "selector.fit(tdidf_features_train, y_train)\n",
    "\n",
    "tdidf_features_train = selector.transform(tdidf_features_train).astype('float32')\n",
    "print('Training Matrix:',tdidf_features_train.shape)\n",
    "\n",
    "tdidf_features_test = selector.transform(tdidf_features_test).astype('float32')\n",
    "print('Test Matrix:',tdidf_features_test.shape)\n",
    "\n",
    "tdidf_features_validation = selector.transform(tdidf_features_validation).astype('float32')\n",
    "print('Validation Matrix:',tdidf_features_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {\n",
    "    'Singapore': 1,\n",
    "    'Sports': 2,\n",
    "    'Lifestyle': 3,\n",
    "    'World': 4,\n",
    "    'Business': 5,\n",
    "    'Technology': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'Business' category:\n",
      "  . Most correlated unigrams:\n",
      ". market\n",
      ". tariff\n",
      ". per\n",
      ". cent\n",
      ". trade\n",
      "  . Most correlated bigrams:\n",
      ". trade war\n",
      ". point per\n",
      ". tariff us\n",
      ". us billion\n",
      ". per cent\n",
      "\n",
      "# 'Lifestyle' category:\n",
      "  . Most correlated unigrams:\n",
      ". story\n",
      ". singer\n",
      ". actor\n",
      ". movie\n",
      ". film\n",
      "  . Most correlated bigrams:\n",
      ". movie review\n",
      ". post share\n",
      ". view post\n",
      ". post instagram\n",
      ". relate story\n",
      "\n",
      "# 'Singapore' category:\n",
      "  . Most correlated unigrams:\n",
      ". tan\n",
      ". ica\n",
      ". mr\n",
      ". jail\n",
      ". singapore\n",
      "  . Most correlated bigrams:\n",
      ". civil defence\n",
      ". years jail\n",
      ". years fin\n",
      ". mr tan\n",
      ". mr lee\n",
      "\n",
      "# 'Sports' category:\n",
      "  . Most correlated unigrams:\n",
      ". cup\n",
      ". champion\n",
      ". match\n",
      ". league\n",
      ". win\n",
      "  . Most correlated bigrams:\n",
      ". last season\n",
      ". premier league\n",
      ". grand slam\n",
      ". us open\n",
      ". world cup\n",
      "\n",
      "# 'Technology' category:\n",
      "  . Most correlated unigrams:\n",
      ". apple\n",
      ". users\n",
      ". battery\n",
      ". percent\n",
      ". huawei\n",
      "  . Most correlated bigrams:\n",
      ". percent us\n",
      ". power bank\n",
      ". artificial intelligence\n",
      ". fake news\n",
      ". battery life\n",
      "\n",
      "# 'World' category:\n",
      "  . Most correlated unigrams:\n",
      ". kashmir\n",
      ". kong\n",
      ". hong\n",
      ". protest\n",
      ". protesters\n",
      "  . Most correlated bigrams:\n",
      ". hong kongs\n",
      ". fire tear\n",
      ". water cannon\n",
      ". tear gas\n",
      ". hong kong\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "for Product, category_id in sorted(category_mapping.items()):\n",
    "    features_chi2 = chi2(tdidf_features_train, y_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(vectorizer.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-5:])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Features\n",
    "with open('Pickles/tdidf_training_features.pickle', 'wb') as output:\n",
    "    pickle.dump(tdidf_features_train, output, protocol=4)\n",
    "    \n",
    "#Training Labels\n",
    "with open('Pickles/tdidf_training_labels.pickle', 'wb') as output:\n",
    "    pickle.dump(y_train, output, protocol=4)\n",
    "    \n",
    "#Test Features\n",
    "with open('Pickles/tdidf_test_features.pickle', 'wb') as output:\n",
    "    pickle.dump(tdidf_features_test, output, protocol=4)\n",
    "    \n",
    "#Test Labels\n",
    "with open('Pickles/tdidf_test_labels.pickle', 'wb') as output:\n",
    "    pickle.dump(y_test, output, protocol=4)\n",
    "    \n",
    "#Test Features\n",
    "with open('Pickles/tdidf_validation_features.pickle', 'wb') as output:\n",
    "    pickle.dump(tdidf_features_validation, output, protocol=4)\n",
    "    \n",
    "#Test Labels\n",
    "with open('Pickles/tdidf_validation_labels.pickle', 'wb') as output:\n",
    "    pickle.dump(y_validation, output, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization parameters\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary with training texts.\n",
    "tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4717], [3353], [4213], [4213], [], [], [7556], [], [4784], [4213], []]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn vocabulary from training texts and vectorize training texts.\n",
    "sv_features_train = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "# Vectorize testing texts.\n",
    "sv_features_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Vectorize validation texts.\n",
    "sv_features_validation = tokenizer.texts_to_sequences(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max sequence length.\n",
    "max_length = len(max(sv_features_train, key=len))\n",
    "if max_length > MAX_SEQUENCE_LENGTH:\n",
    "    max_length = MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix sequence length to max value. Sequences shorter than the length are\n",
    "# padded in the beginning and sequences longer are truncated\n",
    "# at the beginning.\n",
    "sv_features_train = sequence.pad_sequences(sv_features_train, maxlen=max_length)\n",
    "sv_features_test = sequence.pad_sequences(sv_features_test, maxlen=max_length)\n",
    "sv_features_validation = sequence.pad_sequences(sv_features_validation, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Vector Training Matrix: (2400, 1000)\n",
      "Sequence Vector Testing Matrix: (300, 1000)\n",
      "Sequence Vector Validation Matrix: (300, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('Sequence Vector Training Matrix:',sv_features_train.shape)\n",
    "print('Sequence Vector Testing Matrix:',sv_features_test.shape)\n",
    "print('Sequence Vector Validation Matrix:',sv_features_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Features\n",
    "with open('Pickles/sv_training_features.pickle', 'wb') as output:\n",
    "    pickle.dump(sv_features_train, output, protocol=4)\n",
    "    \n",
    "#Training Labels\n",
    "with open('Pickles/sv_training_labels.pickle', 'wb') as output:\n",
    "    pickle.dump(y_train, output, protocol=4)\n",
    "    \n",
    "#Test Features\n",
    "with open('Pickles/sv_test_features.pickle', 'wb') as output:\n",
    "    pickle.dump(sv_features_test, output, protocol=4)\n",
    "    \n",
    "#Test Labels\n",
    "with open('Pickles/sv_test_labels.pickle', 'wb') as output:\n",
    "    pickle.dump(y_test, output, protocol=4)\n",
    "    \n",
    "#Test Features\n",
    "with open('Pickles/sv_validation_features.pickle', 'wb') as output:\n",
    "    pickle.dump(sv_features_validation, output, protocol=4)\n",
    "    \n",
    "#Test Labels\n",
    "with open('Pickles/sv_validation_labels.pickle', 'wb') as output:\n",
    "    pickle.dump(y_validation, output, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

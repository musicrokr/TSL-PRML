{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction & Train Test Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words and Sequence Vectors Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df = \"./Pickles/all_articles_processed.pickle\"\n",
    "\n",
    "with open(path_df, 'rb') as data:\n",
    "    articles = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>category</th>\n",
       "      <th>length_characters</th>\n",
       "      <th>length_words</th>\n",
       "      <th>category_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AsiaOne</td>\n",
       "      <td>7 factors to consider when looking for an HDB ...</td>\n",
       "      <td>whether youre buy hdb resale flat firsttimer l...</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>9771</td>\n",
       "      <td>1769</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Straits Times</td>\n",
       "      <td>Jung Joon-young first to be charged in K-pop s...</td>\n",
       "      <td>seoul  first arrest kpop scandal singer jung ...</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>2162</td>\n",
       "      <td>324</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Straits Times</td>\n",
       "      <td>Music mogul Dr Dre gets flak over boast that h...</td>\n",
       "      <td>los angeles  look talk lack class netizens im...</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>1108</td>\n",
       "      <td>182</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Channel News Asia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>think colonel sanders  quirky enough apparentl...</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>1193</td>\n",
       "      <td>207</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Channel News Asia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mexican government question louis vuittons use...</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>1484</td>\n",
       "      <td>231</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              source                                              title  \\\n",
       "0            AsiaOne  7 factors to consider when looking for an HDB ...   \n",
       "1  The Straits Times  Jung Joon-young first to be charged in K-pop s...   \n",
       "2  The Straits Times  Music mogul Dr Dre gets flak over boast that h...   \n",
       "3  Channel News Asia                                                NaN   \n",
       "4  Channel News Asia                                                NaN   \n",
       "\n",
       "                                             article   category  \\\n",
       "0  whether youre buy hdb resale flat firsttimer l...  Lifestyle   \n",
       "1   seoul  first arrest kpop scandal singer jung ...  Lifestyle   \n",
       "2   los angeles  look talk lack class netizens im...  Lifestyle   \n",
       "3  think colonel sanders  quirky enough apparentl...  Lifestyle   \n",
       "4  mexican government question louis vuittons use...  Lifestyle   \n",
       "\n",
       "   length_characters  length_words  category_code  \n",
       "0               9771          1769              3  \n",
       "1               2162           324              3  \n",
       "2               1108           182              3  \n",
       "3               1193           207              3  \n",
       "4               1484           231              3  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(articles['article'], articles['category_code'], test_size=0.2, random_state=1)\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles in Training Dataset: 5760 80.0 %\n",
      "Articles in Testing Dataset: 720 10.0 %\n",
      "Articles in Validation Dataset: 720 10.0 %\n",
      "Total Number of Articles: 7200\n"
     ]
    }
   ],
   "source": [
    "all_article_count = len(X_train)+len(X_test)+len(X_validation)\n",
    "print('Articles in Training Dataset:',len(X_train),round(len(X_train)/all_article_count,2)*100,\"%\")\n",
    "print('Articles in Testing Dataset:',len(X_test),round(len(X_test)/all_article_count,2)*100,\"%\")\n",
    "print('Articles in Validation Dataset:',len(X_validation),round(len(X_validation)/all_article_count,2)*100,\"%\")\n",
    "print('Total Number of Articles:',all_article_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf Encoding (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 1000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "# Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "kwargs = {\n",
    "        'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "        'dtype': 'int32',\n",
    "        'strip_accents': 'unicode',\n",
    "        'decode_error': 'replace',\n",
    "        'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "        'max_features': TOP_K,\n",
    "        'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1616: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5760, 1000)\n",
      "(720, 1000)\n",
      "(720, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Learn vocabulary from training texts and vectorize training texts.\n",
    "tdidf_features_train = vectorizer.fit_transform(X_train).toarray()\n",
    "print(tdidf_features_train.shape)\n",
    "\n",
    "# Vectorize testing texts.\n",
    "tdidf_features_test = vectorizer.transform(X_test).toarray()\n",
    "print(tdidf_features_test.shape)\n",
    "\n",
    "# Vectorize validation texts.\n",
    "tdidf_features_validation = vectorizer.transform(X_validation).toarray()\n",
    "print(tdidf_features_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5760,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select top 'k' of the vectorized features.\n",
    "selector = SelectKBest(f_classif, k=min(TOP_K, tdidf_features_train.shape[1]))\n",
    "selector.fit(tdidf_features_train, y_train)\n",
    "\n",
    "tdidf_features_train = selector.transform(tdidf_features_train).astype('float32')\n",
    "print('Training Matrix:',tdidf_features_train.shape)\n",
    "\n",
    "tdidf_features_test = selector.transform(tdidf_features_test).astype('float32')\n",
    "print('Test Matrix:',tdidf_features_test.shape)\n",
    "\n",
    "tdidf_features_validation = selector.transform(tdidf_features_validation).astype('float32')\n",
    "print('Validation Matrix:',tdidf_features_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {\n",
    "    'Singapore': 1,\n",
    "    'Sports': 2,\n",
    "    'Lifestyle': 3,\n",
    "    'World': 4,\n",
    "    'Business': 5,\n",
    "    'Technology': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'Business' category:\n",
      "  . Most correlated unigrams:\n",
      ". billion\n",
      ". tariff\n",
      ". trade\n",
      ". per\n",
      ". cent\n",
      "  . Most correlated bigrams:\n",
      ". chief executive\n",
      ". president donald\n",
      ". trade war\n",
      ". us billion\n",
      ". per cent\n",
      "\n",
      "# 'Lifestyle' category:\n",
      "  . Most correlated unigrams:\n",
      ". story\n",
      ". actor\n",
      ". movie\n",
      ". singer\n",
      ". film\n",
      "  . Most correlated bigrams:\n",
      ". us million\n",
      ". post share\n",
      ". view post\n",
      ". post instagram\n",
      ". relate story\n",
      "\n",
      "# 'Singapore' category:\n",
      "  . Most correlated unigrams:\n",
      ". man\n",
      ". singaporeans\n",
      ". mr\n",
      ". jail\n",
      ". singapore\n",
      "  . Most correlated bigrams:\n",
      ". world cup\n",
      ". new york\n",
      ". us billion\n",
      ". say mr\n",
      ". straits time\n",
      "\n",
      "# 'Sports' category:\n",
      "  . Most correlated unigrams:\n",
      ". champion\n",
      ". match\n",
      ". cup\n",
      ". league\n",
      ". win\n",
      "  . Most correlated bigrams:\n",
      ". hong kong\n",
      ". us billion\n",
      ". per cent\n",
      ". us open\n",
      ". world cup\n",
      "\n",
      "# 'Technology' category:\n",
      "  . Most correlated unigrams:\n",
      ". apple\n",
      ". facebook\n",
      ". network\n",
      ". huawei\n",
      ". percent\n",
      "  . Most correlated bigrams:\n",
      ". relate stories\n",
      ". social media\n",
      ". world cup\n",
      ". relate story\n",
      ". us billion\n",
      "\n",
      "# 'World' category:\n",
      "  . Most correlated unigrams:\n",
      ". military\n",
      ". trump\n",
      ". police\n",
      ". protest\n",
      ". protesters\n",
      "  . Most correlated bigrams:\n",
      ". world cup\n",
      ". president donald\n",
      ". donald trump\n",
      ". prime minister\n",
      ". hong kong\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "for Product, category_id in sorted(category_mapping.items()):\n",
    "    features_chi2 = chi2(tdidf_features_train, y_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(vectorizer.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-5:])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Features\n",
    "with open('Pickles/tdidf_training_features.pickle', 'wb') as output:\n",
    "    pickle.dump(tdidf_features_train, output, protocol=4)\n",
    "    \n",
    "#Training Labels\n",
    "with open('Pickles/tdidf_training_labels.pickle', 'wb') as output:\n",
    "    pickle.dump(y_train, output, protocol=4)\n",
    "    \n",
    "#Test Features\n",
    "with open('Pickles/tdidf_test_features.pickle', 'wb') as output:\n",
    "    pickle.dump(tdidf_features_test, output, protocol=4)\n",
    "    \n",
    "#Test Labels\n",
    "with open('Pickles/tdidf_test_labels.pickle', 'wb') as output:\n",
    "    pickle.dump(y_test, output, protocol=4)\n",
    "    \n",
    "#Test Features\n",
    "with open('Pickles/tdidf_validation_features.pickle', 'wb') as output:\n",
    "    pickle.dump(tdidf_features_validation, output, protocol=4)\n",
    "    \n",
    "#Test Labels\n",
    "with open('Pickles/tdidf_validation_labels.pickle', 'wb') as output:\n",
    "    pickle.dump(y_validation, output, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization parameters\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 1000\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary with training texts.\n",
    "tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [], [], [], [], [], [], [], [], []]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn vocabulary from training texts and vectorize training texts.\n",
    "sv_features_train = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "# Vectorize testing texts.\n",
    "sv_features_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Vectorize validation texts.\n",
    "sv_features_validation = tokenizer.texts_to_sequences(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max sequence length.\n",
    "max_length = len(max(sv_features_train, key=len))\n",
    "if max_length > MAX_SEQUENCE_LENGTH:\n",
    "    max_length = MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix sequence length to max value. Sequences shorter than the length are\n",
    "# padded in the beginning and sequences longer are truncated\n",
    "# at the beginning.\n",
    "sv_features_train = sequence.pad_sequences(sv_features_train, maxlen=max_length)\n",
    "sv_features_test = sequence.pad_sequences(sv_features_test, maxlen=max_length)\n",
    "sv_features_validation = sequence.pad_sequences(sv_features_validation, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Vector Training Matrix: (5760, 1000)\n",
      "Sequence Vector Testing Matrix: (720, 1000)\n",
      "Sequence Vector Validation Matrix: (720, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('Sequence Vector Training Matrix:',sv_features_train.shape)\n",
    "print('Sequence Vector Testing Matrix:',sv_features_test.shape)\n",
    "print('Sequence Vector Validation Matrix:',sv_features_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Features\n",
    "with open('Pickles/sv_training_features.pickle', 'wb') as output:\n",
    "    pickle.dump(sv_features_train, output, protocol=4)\n",
    "    \n",
    "#Training Labels\n",
    "with open('Pickles/sv_training_labels.pickle', 'wb') as output:\n",
    "    pickle.dump(y_train, output, protocol=4)\n",
    "    \n",
    "#Test Features\n",
    "with open('Pickles/sv_test_features.pickle', 'wb') as output:\n",
    "    pickle.dump(sv_features_test, output, protocol=4)\n",
    "    \n",
    "#Test Labels\n",
    "with open('Pickles/sv_test_labels.pickle', 'wb') as output:\n",
    "    pickle.dump(y_test, output, protocol=4)\n",
    "    \n",
    "#Test Features\n",
    "with open('Pickles/sv_validation_features.pickle', 'wb') as output:\n",
    "    pickle.dump(sv_features_validation, output, protocol=4)\n",
    "    \n",
    "#Test Labels\n",
    "with open('Pickles/sv_validation_labels.pickle', 'wb') as output:\n",
    "    pickle.dump(y_validation, output, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

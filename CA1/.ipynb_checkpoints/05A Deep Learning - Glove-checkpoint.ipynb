{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.base import get_data_home\n",
    "from keras.metrics import categorical_accuracy\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data Frame\n",
    "data_path = \"./Pickles/articles.pickle\"\n",
    "with open(data_path, 'rb') as data:\n",
    "    df = pickle.load(data)\n",
    "    \n",
    "#Training Features\n",
    "training_features_path = \"./Pickles/training_features.pickle\"\n",
    "with open(training_features_path, 'rb') as data:\n",
    "    training_features = pickle.load(data)\n",
    "    \n",
    "#Training Labels\n",
    "training_labels_path = \"./Pickles/training_labels.pickle\"\n",
    "with open(training_labels_path, 'rb') as data:\n",
    "    training_labels = pickle.load(data)\n",
    "    \n",
    "#Test Features\n",
    "test_features_path = \"./Pickles/test_features.pickle\"\n",
    "with open(test_features_path, 'rb') as data:\n",
    "    test_features = pickle.load(data)\n",
    "    \n",
    "#Test Labels\n",
    "test_labels_path = \"./Pickles/test_labels.pickle\"\n",
    "with open(test_labels_path, 'rb') as data:\n",
    "    test_labels = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>category</th>\n",
       "      <th>Category_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SINGAPORE - A man's body was found on the grou...</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>London (AFP) - Thai Formula One driver Alexand...</td>\n",
       "      <td>Sport</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SINGAPORE - The Straits Times bagged eight awa...</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ready for a challenge? Try out daily Sudoku an...</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HONG KONG (BLOOMBERG) – Hong Kong airport auth...</td>\n",
       "      <td>World</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article   category Category_Code\n",
       "0  SINGAPORE - A man's body was found on the grou...  Singapore             3\n",
       "1  London (AFP) - Thai Formula One driver Alexand...      Sport             4\n",
       "2  SINGAPORE - The Straits Times bagged eight awa...  Singapore             3\n",
       "3  Ready for a challenge? Try out daily Sudoku an...  Lifestyle             0\n",
       "4  HONG KONG (BLOOMBERG) – Hong Kong airport auth...      World             2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Singapore', 'Sport', 'Lifestyle', 'World', 'Business', 'Tech'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6608"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['article'] #Extract text\n",
    "target = df['category'] # Extract target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Singapore\n",
      "1        Sport\n",
      "2    Singapore\n",
      "3    Lifestyle\n",
      "4        World\n",
      "5        World\n",
      "6    Singapore\n",
      "7        World\n",
      "8    Singapore\n",
      "9    Lifestyle\n",
      "Name: category, dtype: object\n",
      "6608\n",
      "6608\n",
      "0    SINGAPORE - A man's body was found on the grou...\n",
      "0    The Nanyang Technological University (NTU) is ...\n",
      "0    BANGKOK: Thailand plans to distribute about 10...\n",
      "Name: article, dtype: object\n",
      "0    Singapore\n",
      "0    Singapore\n",
      "0        World\n",
      "Name: category, dtype: object\n",
      "0    Singapore\n",
      "0    Singapore\n",
      "0        World\n",
      "Name: category, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (target[:10])\n",
    "\n",
    "print (len(texts))\n",
    "print (len(target))\n",
    "#print (len(texts[0].split()))\n",
    "print (texts[0])\n",
    "print (target[0])\n",
    "print (df['category'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
    "tokenizer.fit_on_texts(df['article'])\n",
    "sequences = tokenizer.texts_to_sequences(df['article']) # Generate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16290, 2277, 125, 23, 80]]\n",
      "6608\n",
      "101\n",
      "[35, 5, 4339, 456, 13, 167, 7, 1, 770, 1331, 7541, 18608, 360, 15, 1213, 2173, 2745, 141, 7, 155, 1388, 90, 255, 1, 75, 10, 38, 37, 3185, 2, 5, 280, 4, 10572, 502, 15, 255, 10916, 5, 2746, 36, 106, 206, 13, 167, 9423, 3, 13, 5945, 1362, 16, 9664, 15, 1, 1249, 1, 75, 10, 7972, 10235, 397, 10, 62, 3211, 1, 10573, 360, 51, 62, 4340, 308, 1, 1213, 303, 397, 855, 101, 9, 62, 827, 1, 1292, 4578, 2026, 167, 1, 206, 51, 38, 1092, 1, 1760, 6, 1, 608, 1, 75, 23, 2679, 1, 581]\n"
     ]
    }
   ],
   "source": [
    "print (tokenizer.texts_to_sequences(['Hello King, how are you?']))\n",
    "\n",
    "print (len(sequences))\n",
    "print (len(sequences[0]))\n",
    "print (sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 59,515 unique words.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found {:,} unique words.'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singapore a man's body was found on the ground floor rubbish chute area at block woodlands avenue 6 on monday afternoon aug 12 the police said they were alerted to a case of unnatural death at 12 05pm a 64 year old man was found motionless and was pronounced dead by paramedics at the scene the police said photographer vivian low said she noticed the cordoned area when she walked past the block ms low 29 added that she heard the housing cleaning crew found the man when they opened the door in the morning the police are investigating the incident "
     ]
    }
   ],
   "source": [
    "# Create inverse index mapping numbers to words\n",
    "inv_index = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "# Print out text again\n",
    "for w in sequences[0]:\n",
    "    x = inv_index.get(w)\n",
    "    print(x,end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(441.43674334140434, 351.2286768877622)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the average length of a text\n",
    "avg = sum(map(len, sequences)) / len(sequences)\n",
    "\n",
    "# Get the standard deviation of the sequence length\n",
    "std = np.sqrt(sum(map(lambda x: (len(x) - avg)**2, sequences)) / len(sequences))\n",
    "\n",
    "avg,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 2 3]]\n",
      "[[2 3 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "print(pad_sequences([[1,2,3]], maxlen=5))\n",
    "print(pad_sequences([[1,2,3,4,5,6]], maxlen=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "data = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (6608, 100)\n",
      "Shape of labels: (6608, 6)\n",
      "0    3\n",
      "0    3\n",
      "0    2\n",
      "Name: Category_Code, dtype: object\n",
      "[0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(np.asarray(df['Category_Code']))\n",
    "print('Shape of data:', data.shape)\n",
    "print('Shape of labels:', labels.shape)\n",
    "\n",
    "print (df['Category_Code'][0])\n",
    "print (labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400,000 word vectors in GloVe.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = './Input/Glove' # This is the folder with the dataset\n",
    "\n",
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "        embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "        embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n",
    "\n",
    "print('Found {:,} word vectors in GloVe.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.043084   0.53233    0.54254   -0.076952  -0.29673    0.52986\n",
      "  0.21379    0.15789   -0.3952    -0.91889   -0.6585     0.68706\n",
      "  0.10821   -0.10694   -0.3401     1.044      0.12775    0.51157\n",
      "  0.60314    0.71366   -0.5374     0.37737    0.12186    0.60891\n",
      "  0.50107    2.0215    -0.47318    0.46953    0.12542    0.60207\n",
      "  0.11007    0.37587    1.0137    -0.2478     0.65748    0.12801\n",
      " -0.57647   -0.25754    0.62426    0.010864  -0.40681    0.16173\n",
      " -0.84695   -0.24603    0.29078    0.8546    -0.067021   0.69331\n",
      " -0.71545   -0.25184   -0.74741   -0.26507    0.4873     0.41991\n",
      " -0.86741   -0.5235    -0.44774   -0.044584   0.033836   0.29909\n",
      "  0.73754    0.81651    0.69431    0.80453    0.29276   -0.025244\n",
      " -0.30453   -0.34329    0.11933   -0.29655    0.1072    -0.18946\n",
      "  0.18501   -0.7548    -0.25628    0.34438   -0.016743   0.0040503\n",
      "  0.39342    0.99404   -0.32159   -0.49434    0.41708   -0.011019\n",
      " -0.16613   -0.20839    0.28152   -0.82996    0.79839    0.61645\n",
      "  0.31537   -0.27629   -0.54592    0.23026    0.023473  -0.15934\n",
      " -1.4389    -0.75359    0.5149    -0.52552  ]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print (embeddings_index['frog'])\n",
    "print (len(embeddings_index['frog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.364068\n",
      "5.197995\n",
      "4.1249743\n",
      "6.7943544\n",
      "7.3115597\n",
      "6.5261197\n",
      "7.450874\n"
     ]
    }
   ],
   "source": [
    "print (np.linalg.norm(embeddings_index['man'] - embeddings_index['woman']))\n",
    "print (np.linalg.norm(embeddings_index['man'] - embeddings_index['cat']))\n",
    "\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['toad']))\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['man']))\n",
    "\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['fog']))\n",
    "\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['fork']))\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['skyscraper']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100 # We use 100 dimensional glove vectors\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "# The vectors need to be in the same position as their index. \n",
    "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "\n",
    "# Loop over all words in the word index\n",
    "for word, i in word_index.items():\n",
    "    # If we are above the amount of words we want to use we do nothing\n",
    "    if i >= vocab_size: \n",
    "        continue\n",
    "    # Get the embedding vector for the word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # If there is an embedding vector, put it in the embedding matrix\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.055093    0.87409002 -0.61453003  0.41686001 -0.17532     0.41532001\n",
      "  0.67079002  0.31231999  0.11518     0.44578999  0.98032999  0.27021\n",
      "  0.32286999 -0.41687     0.081158   -0.52557999 -0.071021   -0.17105\n",
      " -0.23309     0.081655    1.00670004  0.27621999  0.35778001  0.34959999\n",
      "  0.48432001 -0.75914001  1.31200004  0.21229     0.41800001 -0.18100999\n",
      " -0.37250999  0.22969    -0.45206001  0.16291    -0.36039001 -0.020007\n",
      "  0.094009    0.27473    -0.65008998  0.20959     0.18533    -0.35104999\n",
      "  0.35942    -0.39127001  0.59227002 -0.68954998 -0.61781001 -0.68761998\n",
      " -0.62682003  0.18971001 -0.28650001 -0.83964998  0.221       0.56274003\n",
      " -0.57836998  0.36058    -0.18009    -0.90030998  0.66766    -0.49981001\n",
      "  0.30066001  0.81476998 -0.42561999 -0.050868    0.18545     0.56084001\n",
      " -0.50190002 -0.14238     1.64699996 -0.26067001  0.32367     0.37843999\n",
      " -0.23212001  0.51011997  0.94016999  0.33790001  0.46197     0.78785002\n",
      " -0.30884001  0.0098883   0.87519002 -0.20475    -0.24078999 -0.57887\n",
      " -0.30462     0.49680001 -0.061581   -0.43378001  0.19138999  0.62347001\n",
      "  0.083169    0.78157997  0.20574    -0.94222999 -0.62217999 -0.065278\n",
      " -0.53332001  0.48194     0.3504     -0.53986001]\n"
     ]
    }
   ],
   "source": [
    "print (embedding_matrix[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, \n",
    "                        embedding_dim, \n",
    "                        input_length=max_length, \n",
    "                        weights = [embedding_matrix], \n",
    "                        trainable = False))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Setup the models\n",
    "model       = createModel() # This is meant for training\n",
    "modelGo     = createModel() # This is used for final testing\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname   = 'deeplearning_glove'\n",
    "filepath        = modelname + \".hdf5\"\n",
    "checkpoint      = ModelCheckpoint(filepath, \n",
    "                                  monitor='val_acc', \n",
    "                                  verbose=0, \n",
    "                                  save_best_only=True, \n",
    "                                  mode='max')\n",
    "\n",
    "                            # Log the epoch detail into csv\n",
    "csv_logger      = CSVLogger('./Models/'+modelname +'.csv')\n",
    "callbacks_list  = [checkpoint,csv_logger]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trDat, tsDat, trLbl, tsLbl = train_test_split(data, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',  # https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#model.fit(data, labels, validation_split=0.2, epochs=10)\n",
    "\n",
    "\n",
    "model.fit(trDat, \n",
    "          trLbl, \n",
    "          validation_data=(tsDat, tsLbl), \n",
    "          epochs=10, \n",
    "          batch_size=128,\n",
    "          shuffle=True,\n",
    "          callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts    = modelGo.predict(tsDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predout     = np.argmax(predicts,axis=1)\n",
    "testout     = np.argmax(tsLbl,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelname   = ['Singapore', 'Sport', 'Lifestyle', 'World', 'Business', 'Tech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testScores  = metrics.accuracy_score(testout,predout)\n",
    "confusion   = metrics.confusion_matrix(testout,predout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best accuracy (on testing dataset): %.2f%%\" % (testScores*100))\n",
    "print(metrics.classification_report(testout,predout,target_names=labelname,digits=4))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "records     = pd.read_csv('./Models/'+modelname +'.csv')\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.plot(records['val_loss'])\n",
    "plt.plot(records['loss'])\n",
    "plt.yticks([0,0.20,0.40,0.60,0.80,1.00])\n",
    "plt.title('Loss value',fontsize=12)\n",
    "\n",
    "ax          = plt.gca()\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(records['val_acc'])\n",
    "plt.plot(records['acc'])\n",
    "plt.yticks([0.6,0.7,0.8,0.9,1.0])\n",
    "plt.title('Accuracy',fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = data[400] # get the tokens\n",
    "print (df['article'][400])\n",
    "\n",
    "# Print tokens as text\n",
    "for w in example:\n",
    "    x = inv_index.get(w)\n",
    "    print(x,end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction\n",
    "pred = model.predict(example.reshape(1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output predicted category\n",
    "df['category'][np.argmax(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Models/decisionTree_best_model.pickle', 'wb') as output:\n",
    "    pickle.dump(best_lrc, output)\n",
    "    \n",
    "with open('Models/decisionTree_best_model_details.pickle', 'wb') as output:\n",
    "    pickle.dump(df_models_lrc, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

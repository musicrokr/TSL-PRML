{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/towards-artificial-intelligence/text-classification-by-xgboost-others-a-case-study-using-bbc-news-articles-5d88e94a9f8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processed Data Frame [ Do Not Use this, Use the Training, Tesing and Validation Data Sets Instead]\n",
    "data_path = \"./Pickles/all_articles_processed.pickle\"\n",
    "with open(data_path, 'rb') as data:\n",
    "    all_articles = pickle.load(data)\n",
    "    \n",
    "    \n",
    "#TD-IDF Features    \n",
    "#Training Features\n",
    "training_features_path = \"./Pickles/tdidf_training_features.pickle\"\n",
    "with open(training_features_path, 'rb') as data:\n",
    "    tdidf_training_features = pickle.load(data)\n",
    "    \n",
    "#Training Labels\n",
    "training_labels_path = \"./Pickles/tdidf_training_labels.pickle\"\n",
    "with open(training_labels_path, 'rb') as data:\n",
    "    tdidf_training_labels = pickle.load(data)\n",
    "    \n",
    "#Test Features\n",
    "test_features_path = \"./Pickles/tdidf_test_features.pickle\"\n",
    "with open(test_features_path, 'rb') as data:\n",
    "    tdidf_test_features = pickle.load(data)\n",
    "    \n",
    "#Test Labels\n",
    "test_labels_path = \"./Pickles/tdidf_test_labels.pickle\"\n",
    "with open(test_labels_path, 'rb') as data:\n",
    "    tdidf_test_labels = pickle.load(data)\n",
    "    \n",
    "#Validation Features\n",
    "test_features_path = \"./Pickles/tdidf_validation_features.pickle\"\n",
    "with open(test_features_path, 'rb') as data:\n",
    "    tdidf_validation_features = pickle.load(data)\n",
    "    \n",
    "#Validation Labels\n",
    "test_labels_path = \"./Pickles/tdidf_validation_labels.pickle\"\n",
    "with open(test_labels_path, 'rb') as data:\n",
    "    tdidf_validation_labels = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5760, 1000)\n",
      "(5760,)\n"
     ]
    }
   ],
   "source": [
    "print(tdidf_training_features.shape)\n",
    "print(tdidf_training_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1000)\n",
      "(720,)\n"
     ]
    }
   ],
   "source": [
    "print(tdidf_test_features.shape)\n",
    "print(tdidf_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gb_base_model = GradientBoostingClassifier(random_state = 8)\n",
    "gb_base_model.fit(tdidf_training_features, tdidf_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Baseline Accuracy: 0.7930555555555555\n"
     ]
    }
   ],
   "source": [
    "gb_base_model_score = accuracy_score(tdidf_test_labels, gb_base_model.predict(tdidf_test_features))\n",
    "print(\"Gradient Boosting Baseline Accuracy:\", gb_base_model_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_base_model = xgb.XGBClassifier(random_state = 8)\n",
    "xgb_base_model.fit(tdidf_training_features, tdidf_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Baseline Accuracy: 0.8055555555555556\n"
     ]
    }
   ],
   "source": [
    "xgb_base_model_score = accuracy_score(tdidf_test_labels, xgb_base_model.predict(tdidf_test_features))\n",
    "print(\"Gradient Boosting Baseline Accuracy:\", xgb_base_model_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': [0.1, 0.5],\n",
      " 'max_depth': [10, 40, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [10, 30, 50],\n",
      " 'n_estimators': [200, 800],\n",
      " 'subsample': [0.5, 1.0]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# n_estimators\n",
    "n_estimators = [200, 800]\n",
    "\n",
    "# max_features\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# max_depth\n",
    "max_depth = [10, 40]\n",
    "max_depth.append(None)\n",
    "\n",
    "# min_samples_split\n",
    "min_samples_split = [10, 30, 50]\n",
    "\n",
    "# min_samples_leaf\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# learning rate\n",
    "learning_rate = [.1, .5]\n",
    "\n",
    "# subsample\n",
    "subsample = [.5, 1.]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'learning_rate': learning_rate,\n",
    "               'subsample': subsample}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# First create the base model to tune\n",
    "gbc = GradientBoostingClassifier(random_state=8)\n",
    "\n",
    "# Definition of the random search\n",
    "random_search = RandomizedSearchCV(estimator=gbc,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=50,\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   random_state=8)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(tdidf_validation_features, tdidf_validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best hyperparameters from Random Search are:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "max_depth = [5, 10, 15]\n",
    "max_features = ['sqrt']\n",
    "min_samples_leaf = [2]\n",
    "min_samples_split = [50, 100]\n",
    "n_estimators = [800]\n",
    "learning_rate = [.1, .5]\n",
    "subsample = [1.]\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': max_depth,\n",
    "    'max_features': max_features,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'n_estimators': n_estimators,\n",
    "    'learning_rate': learning_rate,\n",
    "    'subsample': subsample\n",
    "\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "gbc = GradientBoostingClassifier(random_state=8)\n",
    "\n",
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "cv_sets = ShuffleSplit(n_splits = 3, test_size = .33, random_state = 8)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=gbc, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(tdidf_validation_features, tdidf_validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best hyperparameters from Grid Search are:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ab_base_model = AdaBoostClassifier(random_state = 8,)\n",
    "ab_base_model.fit(tdidf_training_features, tdidf_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_base_model_score = accuracy_score(tdidf_test_labels, ab_base_model.predict(tdidf_test_features))\n",
    "print(\"AdaBoost Baseline Accuracy:\", ab_base_model_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

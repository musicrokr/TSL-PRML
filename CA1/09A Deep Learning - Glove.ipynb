{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.base import get_data_home\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processed Data Frame [ Do Not Use this, Use the Training, Tesing and Validation Data Sets Instead]\n",
    "data_path = \"./Pickles/all_articles_processed.pickle\"\n",
    "with open(data_path, 'rb') as data:\n",
    "    all_articles = pickle.load(data)\n",
    "    \n",
    "    \n",
    "#TD-IDF Features    \n",
    "#Training Features\n",
    "training_features_path = \"./Pickles/tdidf_training_features.pickle\"\n",
    "with open(training_features_path, 'rb') as data:\n",
    "    tdidf_training_features = pickle.load(data)\n",
    "    \n",
    "#Training Labels\n",
    "training_labels_path = \"./Pickles/tdidf_training_labels.pickle\"\n",
    "with open(training_labels_path, 'rb') as data:\n",
    "    tdidf_training_labels = pickle.load(data)\n",
    "    \n",
    "#Test Features\n",
    "test_features_path = \"./Pickles/tdidf_test_features.pickle\"\n",
    "with open(test_features_path, 'rb') as data:\n",
    "    tdidf_test_features = pickle.load(data)\n",
    "    \n",
    "#Test Labels\n",
    "test_labels_path = \"./Pickles/tdidf_test_labels.pickle\"\n",
    "with open(test_labels_path, 'rb') as data:\n",
    "    tdidf_test_labels = pickle.load(data)\n",
    "    \n",
    "#Validation Features\n",
    "test_features_path = \"./Pickles/tdidf_validation_features.pickle\"\n",
    "with open(test_features_path, 'rb') as data:\n",
    "    tdidf_validation_features = pickle.load(data)\n",
    "    \n",
    "#Validation Labels\n",
    "test_labels_path = \"./Pickles/tdidf_validation_labels.pickle\"\n",
    "with open(test_labels_path, 'rb') as data:\n",
    "    tdidf_validation_labels = pickle.load(data)\n",
    "    \n",
    "    \n",
    "#Sequence Vector Features    \n",
    "#Training Features\n",
    "training_features_path = \"./Pickles/sv_training_features.pickle\"\n",
    "with open(training_features_path, 'rb') as data:\n",
    "    sv_training_features = pickle.load(data)\n",
    "    \n",
    "#Training Labels\n",
    "training_labels_path = \"./Pickles/sv_training_labels.pickle\"\n",
    "with open(training_labels_path, 'rb') as data:\n",
    "    sv_training_labels = pickle.load(data)\n",
    "    \n",
    "#Test Features\n",
    "test_features_path = \"./Pickles/sv_test_features.pickle\"\n",
    "with open(test_features_path, 'rb') as data:\n",
    "    sv_test_features = pickle.load(data)\n",
    "    \n",
    "#Test Labels\n",
    "test_labels_path = \"./Pickles/sv_test_labels.pickle\"\n",
    "with open(test_labels_path, 'rb') as data:\n",
    "    sv_test_labels = pickle.load(data)\n",
    "    \n",
    "#Validation Features\n",
    "test_features_path = \"./Pickles/sv_validation_features.pickle\"\n",
    "with open(test_features_path, 'rb') as data:\n",
    "    sv_validation_features = pickle.load(data)\n",
    "    \n",
    "#Validation Labels\n",
    "test_labels_path = \"./Pickles/sv_validation_labels.pickle\"\n",
    "with open(test_labels_path, 'rb') as data:\n",
    "    sv_validation_labels = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_articles\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['article'] #Extract text\n",
    "target = df['category'] # Extract target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (target[:10])\n",
    "\n",
    "print (len(texts))\n",
    "print (len(target))\n",
    "#print (len(texts[0].split()))\n",
    "print (texts[0])\n",
    "print (target[0])\n",
    "print (df['category'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
    "tokenizer.fit_on_texts(df['article'])\n",
    "sequences = tokenizer.texts_to_sequences(df['article']) # Generate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (tokenizer.texts_to_sequences(['Hello King, how are you?']))\n",
    "\n",
    "print (len(sequences))\n",
    "print (len(sequences[0]))\n",
    "print (sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found {:,} unique words.'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inverse index mapping numbers to words\n",
    "inv_index = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "# Print out text again\n",
    "for w in sequences[0]:\n",
    "    x = inv_index.get(w)\n",
    "    print(x,end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average length of a text\n",
    "avg = sum(map(len, sequences)) / len(sequences)\n",
    "\n",
    "# Get the standard deviation of the sequence length\n",
    "std = np.sqrt(sum(map(lambda x: (len(x) - avg)**2, sequences)) / len(sequences))\n",
    "\n",
    "avg,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pad_sequences([[1,2,3]], maxlen=5))\n",
    "print(pad_sequences([[1,2,3,4,5,6]], maxlen=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "data = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(np.asarray(df['category_code']))\n",
    "print('Shape of data:', data.shape)\n",
    "print('Shape of labels:', labels.shape)\n",
    "\n",
    "print (df['category_code'][0])\n",
    "print (labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = './Glove' # This is the folder with the dataset\n",
    "\n",
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "        embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "        embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n",
    "\n",
    "print('Found {:,} word vectors in GloVe.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (embeddings_index['frog'])\n",
    "print (len(embeddings_index['frog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (np.linalg.norm(embeddings_index['man'] - embeddings_index['woman']))\n",
    "print (np.linalg.norm(embeddings_index['man'] - embeddings_index['cat']))\n",
    "\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['toad']))\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['man']))\n",
    "\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['fog']))\n",
    "\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['fork']))\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['skyscraper']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100 # We use 100 dimensional glove vectors\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "# The vectors need to be in the same position as their index. \n",
    "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "\n",
    "# Loop over all words in the word index\n",
    "for word, i in word_index.items():\n",
    "    # If we are above the amount of words we want to use we do nothing\n",
    "    if i >= vocab_size: \n",
    "        continue\n",
    "    # Get the embedding vector for the word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # If there is an embedding vector, put it in the embedding matrix\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (embedding_matrix[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, \n",
    "                        embedding_dim, \n",
    "                        input_length=max_length, \n",
    "                        weights = [embedding_matrix], \n",
    "                        trainable = True))\n",
    "    model.add(Conv1D(32, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the models\n",
    "model       = createModel() # This is meant for training\n",
    "modelGo     = createModel() # This is used for final testing\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname   = 'deeplearning_glove'\n",
    "filepath        = './Models/'+modelname + \".hdf5\"\n",
    "checkpoint      = ModelCheckpoint(filepath, \n",
    "                                  monitor='val_acc', \n",
    "                                  verbose=0, \n",
    "                                  save_best_only=True, \n",
    "                                  mode='max')\n",
    "\n",
    "                            # Log the epoch detail into csv\n",
    "csv_logger      = CSVLogger('./Models/'+modelname +'.csv')\n",
    "callbacks_list  = [checkpoint,csv_logger]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trDat, tsDat, trLbl, tsLbl = train_test_split(data, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',  # https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#model.fit(data, labels, validation_split=0.2, epochs=10)\n",
    "\n",
    "\n",
    "model.fit(trDat, \n",
    "          trLbl, \n",
    "          validation_data=(tsDat, tsLbl), \n",
    "          epochs=10, \n",
    "          batch_size=128,\n",
    "          shuffle=True,\n",
    "          callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts    = modelGo.predict(tsDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predout     = np.argmax(predicts,axis=1)\n",
    "testout     = np.argmax(tsLbl,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelname   = ['Singapore', 'Sport', 'Lifestyle', 'World', 'Business', 'Tech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testScores  = metrics.accuracy_score(testout,predout)\n",
    "confusion   = metrics.confusion_matrix(testout,predout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best accuracy (on testing dataset): %.2f%%\" % (testScores*100))\n",
    "print(metrics.classification_report(testout,predout,digits=4))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "records     = pd.read_csv('./Models/'+modelname +'.csv')\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.plot(records['val_loss'])\n",
    "plt.plot(records['loss'])\n",
    "plt.yticks([0,0.20,0.40,0.60,0.80,1.00])\n",
    "plt.title('Loss value',fontsize=12)\n",
    "\n",
    "ax          = plt.gca()\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(records['val_acc'])\n",
    "plt.plot(records['acc'])\n",
    "plt.yticks([0.6,0.7,0.8,0.9,1.0])\n",
    "plt.title('Accuracy',fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = data[400] # get the tokens\n",
    "print (df['article'][400])\n",
    "\n",
    "# Print tokens as text\n",
    "for w in example:\n",
    "    x = inv_index.get(w)\n",
    "    print(x,end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction\n",
    "pred = model.predict(example.reshape(1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output predicted category\n",
    "df['category'][np.argmax(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Models/decisionTree_best_model.pickle', 'wb') as output:\n",
    "    pickle.dump(best_lrc, output)\n",
    "    \n",
    "with open('Models/decisionTree_best_model_details.pickle', 'wb') as output:\n",
    "    pickle.dump(df_models_lrc, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = \"/home/lnc/0. Latest News Classifier/04. Model Training/Models/\"\n",
    "\n",
    "# SVM\n",
    "path_svm = path_models + 'best_svc.pickle'\n",
    "with open(path_svm, 'rb') as data:\n",
    "    svc_model = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve TD-IDF Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tfidf = \"/home/lnc/0. Latest News Classifier/03. Feature Engineering/Pickles/tfidf.pickle\"\n",
    "with open(path_tfidf, 'rb') as data:\n",
    "    tfidf = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict Text Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_text(text):\n",
    "    \n",
    "    # Predict using the input model\n",
    "    prediction_svc = svc_model.predict(create_features_from_text(text))[0]\n",
    "    prediction_svc_proba = svc_model.predict_proba(create_features_from_text(text))[0]\n",
    "    \n",
    "    # Return result\n",
    "    category_svc = get_category_name(prediction_svc)\n",
    "    \n",
    "    print(\"The predicted category using the SVM model is %s.\" %(category_svc) )\n",
    "    print(\"The conditional probability is: %a\" %(prediction_svc_proba.max()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# \n",
    "# Comparing Bagging and Boosting Ensemble Methods\n",
    "# Author: D Sotelo\n",
    "# Date 20170708\n",
    "#\n",
    "# Data available via Kaggle.com\n",
    "# https://www.kaggle.com/zynicide/wine-reviews\n",
    "#\n",
    "##########################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read in & munge wine information dataset.  US wines only\n",
    "wine_df=pd.read_csv('winemag-data_first150k.csv')\n",
    "wine_df=wine_df.loc[wine_df.country=='US',['points','price','region_1', \\\n",
    "    'variety','winery']]\n",
    "wine_df=wine_df.dropna(axis=0,how='any')\n",
    "\n",
    "# Map point values to categories\n",
    "bin_map={\n",
    "    100:'90+',\n",
    "    99:'90+',\n",
    "    98:'90+',\n",
    "    97:'90+',\n",
    "    96:'90+',\n",
    "    95:'90+',\n",
    "    94:'90+',\n",
    "    93:'90+',\n",
    "    92:'90+',\n",
    "    91:'90+',\n",
    "    90:'90+',\n",
    "    89:'<90',\n",
    "    88:'<90',\n",
    "    87:'<90',\n",
    "    86:'<90',\n",
    "    85:'<90',\n",
    "    84:'<90',\n",
    "    83:'<90',\n",
    "    82:'<90',\n",
    "    81:'<90',\n",
    "    80:'<90',\n",
    "    79:'<90',\n",
    "    78:'<90',\n",
    "    77:'<90',\n",
    "    76:'<90'}\n",
    "wine_df['point_bins']=wine_df.points.map(bin_map)\n",
    "wine_df.point_bins.unique() # Ensure no records are un-binned\n",
    "wine_df=wine_df.drop('points',axis=1)\n",
    "\n",
    "#Prepare data for classification by labeling category data\n",
    "regn_lab=LabelEncoder().fit(np.unique(wine_df.region_1.values))\n",
    "var_lab=LabelEncoder().fit(np.unique(wine_df.variety.values))\n",
    "wnry_lab=LabelEncoder().fit(np.unique(wine_df.winery.values))\n",
    "wine_df['regn_enc']=regn_lab.transform(wine_df.region_1)\n",
    "wine_df['var_enc']=var_lab.transform(wine_df.variety)\n",
    "wine_df['wnry_enc']=wnry_lab.transform(wine_df.winery)\n",
    "wine_df=wine_df.drop(['region_1','variety','winery'],axis=1)\n",
    "\n",
    "# Split into 70/30 train/test datasets and set up training variables\n",
    "wine_train,wine_test=train_test_split(wine_df,test_size=0.30)\n",
    "x=wine_train.loc[:,['price','regn_enc','var_enc','wnry_enc']]\n",
    "y=wine_train['point_bins']\n",
    "\n",
    "# Train classification tree                \n",
    "clf=DecisionTreeClassifier(max_features=None)\n",
    "clf=clf.fit(x,y)\n",
    "\n",
    "# Report classification results.  training dataset first, then test.  \n",
    "train_error=y==clf.predict(x)\n",
    "test_error=wine_test['point_bins']==clf.predict(wine_test.loc[:,['price', \\\n",
    "    'regn_enc','var_enc','wnry_enc']])\n",
    "print('@@@@@@@@@@@@@@@@@@@@@@@@@')\n",
    "print('CART w/ #leaf nodes = ',clf.tree_.node_count) \n",
    "print('   ',clf.n_features_,' features out of: 4 features')                          \n",
    "print('   training accuracy: ','{:.1%}'.format(sum(train_error)/len(\n",
    "    train_error)))\n",
    "print('   test accuracy: ','{:.1%}'.format(sum(test_error)/len(test_error)))\n",
    "\n",
    "# Control the number of n_estimators in ensemble functions\n",
    "max_n_ests=25\n",
    "\n",
    "# Create dataframe to record results of ensembles.\n",
    "results=pd.DataFrame([],columns=list(['type','n_leaf','n_est', \\\n",
    "    'train_acc','test_acc']))\n",
    "\n",
    "# Train bagging ensemble on iterations of n_estimators=i\n",
    "# and iterations of stump max_leaf_nodes=j\n",
    "for j in [500,2000,8000,99999]:\n",
    "    clf_stump=DecisionTreeClassifier(max_features=None,max_leaf_nodes=j)\n",
    "    for i in np.arange(1,max_n_ests):\n",
    "        print(i)\n",
    "        baglfy=BaggingClassifier(base_estimator=clf_stump,n_estimators=i,\n",
    "            max_samples=1.0)\n",
    "        baglfy=baglfy.fit(x,y)\n",
    "        bag_tr_err=y==baglfy.predict(x)\n",
    "        bag_tst_err=wine_test['point_bins']==baglfy.predict( \\\n",
    "            wine_test.loc[:,['price','regn_enc','var_enc','wnry_enc']])\n",
    "        run_rslt=pd.DataFrame([['bag',j,i,sum(bag_tr_err)/len(bag_tr_err),\n",
    "            sum(bag_tst_err)/len(bag_tst_err)]],\n",
    "            columns=list(['type','n_leaf','n_est','train_acc','test_acc']))\n",
    "        results=results.append(run_rslt)\n",
    "\n",
    "# Train boosting ensemble on iterations of n_estimators=i\n",
    "# and iterations of stump max_leaf_nodes=j\n",
    "for j in [500,2000,8000,99999]:\n",
    "    clf_stump=DecisionTreeClassifier(max_features=None,max_leaf_nodes=j)\n",
    "    for i in np.arange(1,max_n_ests):\n",
    "        print(i)\n",
    "        bstlfy=AdaBoostClassifier(base_estimator=clf_stump,n_estimators=i)\n",
    "        bstlfy=bstlfy.fit(x,y)\n",
    "        bst_tr_err=y==bstlfy.predict(x)\n",
    "        bst_tst_err=wine_test['point_bins']==bstlfy.predict( \\\n",
    "            wine_test.loc[:,['price','regn_enc','var_enc','wnry_enc']])\n",
    "        run_rslt=pd.DataFrame([['bst',j,i,sum(bst_tr_err)/len(bst_tr_err),\n",
    "            sum(bst_tst_err)/len(bst_tst_err)]],\n",
    "            columns=list(['type','n_leaf','n_est','train_acc','test_acc']))\n",
    "        results=results.append(run_rslt)\n",
    "\n",
    "# Plot Bagging accuracy results on test data\n",
    "# 500 leaf stumps\n",
    "plt.plot(results.loc[((results.type=='bag')&(results.n_leaf==500)), \\\n",
    "    ['n_est']],results.loc[((results.type=='bag')&(results.n_leaf==500)), \\\n",
    "    ['test_acc']],linestyle='--',linewidth=2,color='#4da6ff', \\\n",
    "    label='Bagging w/ 500 leaf stump')\n",
    "# 2000 leaf stumps\n",
    "plt.plot(results.loc[((results.type=='bag')&(results.n_leaf==2000)), \\\n",
    "    ['n_est']],results.loc[((results.type=='bag')&(results.n_leaf==2000)), \\\n",
    "    ['test_acc']],linestyle='--',linewidth=2,color='#0080ff', \\\n",
    "    label='Bagging w/ 2000 leaf stump')\n",
    "# 8000 leaf stumps\n",
    "plt.plot(results.loc[((results.type=='bag')&(results.n_leaf==8000)), \\\n",
    "    ['n_est']],results.loc[((results.type=='bag')&(results.n_leaf==8000)), \\\n",
    "    ['test_acc']],linestyle='--',linewidth=2,color='#0059b3', \\\n",
    "    label='Bagging w/ 8000 leaf stump')\n",
    "# Full Classification Trees (no early termination)\n",
    "plt.plot(results.loc[((results.type=='bag')&(results.n_leaf==99999)), \\\n",
    "    ['n_est']],results.loc[((results.type=='bag')&(results.n_leaf==99999)), \\\n",
    "    ['test_acc']],linestyle='--',linewidth=2,color='#003366', \\\n",
    "    label='Bagging w/ full tree')\n",
    "# Plot test accuracy of baseline classification tree\n",
    "clf_test_acc=sum(test_error)/len(test_error)\n",
    "plt.plot([1,max_n_ests],[clf_test_acc,clf_test_acc],color='k', \\\n",
    "    label='Baseline classification tree')\n",
    "plt.legend(fontsize=8)\n",
    "plt.title('Bagging Test Sample Accuracy on n_estimators')\n",
    "plt.ylim([results.loc[results.type=='bag',['test_acc']].values.min()-0.01, \\\n",
    "    results.loc[results.type=='bag',['test_acc']].values.max()+0.01])\n",
    "plt.ylabel('Test Accuracy%')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.show()\n",
    "\n",
    "# Plot Boosting accuracy results on test data\n",
    "# 500 leaf stumps\n",
    "plt.plot(results.loc[((results.type=='bst')&(results.n_leaf==500)), \\\n",
    "    ['n_est']],results.loc[((results.type=='bst')&(results.n_leaf==500)), \\\n",
    "    ['test_acc']],linestyle='--',linewidth=2,color='#ff704d', \\\n",
    "    label='Boosting w/ 500 leaf stump')\n",
    "# 2000 leaf stumps\n",
    "plt.plot(results.loc[((results.type=='bst')&(results.n_leaf==2000)), \\\n",
    "    ['n_est']],results.loc[((results.type=='bst')&(results.n_leaf==2000)), \\\n",
    "    ['test_acc']],linestyle='--',linewidth=2,color='#ff3300', \\\n",
    "    label='Boosting w/ 2000 leaf stump')\n",
    "# 8000 leaf stumps\n",
    "plt.plot(results.loc[((results.type=='bst')&(results.n_leaf==8000)), \\\n",
    "    ['n_est']],results.loc[((results.type=='bst')&(results.n_leaf==8000)), \\\n",
    "    ['test_acc']],linestyle='--',linewidth=2,color='#b32400', \\\n",
    "    label='Boosting w/ 8000 leaf stump')\n",
    "# Full Classification Trees (no early termination)\n",
    "plt.plot(results.loc[((results.type=='bst')&(results.n_leaf==99999)), \\\n",
    "    ['n_est']],results.loc[((results.type=='bst')&(results.n_leaf==99999)), \\\n",
    "    ['test_acc']],linestyle='--',linewidth=2,color='#661400', \\\n",
    "    label='Boosting w/ full tree')\n",
    "# Plot test accuracy of baseline classification tree\n",
    "plt.plot([1,max_n_ests],[clf_test_acc,clf_test_acc],color='k', \\\n",
    "    label='Baseline classification tree')\n",
    "plt.legend(fontsize=8)\n",
    "plt.title('Boosting Test Sample Accuracy on n_estimators')\n",
    "plt.ylim([results.loc[results.type=='bst',['test_acc']].values.min()-0.01, \\\n",
    "    results.loc[results.type=='bst',['test_acc']].values.max()+0.01])\n",
    "plt.ylabel('Test Accuracy%')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.show()\n",
    "\n",
    "# ROC curve for baseline classification tree\n",
    "clf_probs=clf.predict_proba(wine_test.loc[:,['price','regn_enc','var_enc', \\\n",
    "    'wnry_enc']])\n",
    "fpr1,tpr1,thr1=roc_curve(np.where(wine_test['point_bins']=='90+',1.,0.), \\\n",
    "    clf_probs[:,0])\n",
    "# ROC curve for bagging ensemble using full classification trees\n",
    "bag_probs=baglfy.predict_proba(wine_test.loc[:,['price','regn_enc', \\\n",
    "    'var_enc','wnry_enc']])\n",
    "fpr2,tpr2,thr2=roc_curve(np.where(wine_test['point_bins']=='90+',1.,0.), \\\n",
    "    bag_probs[:,0])\n",
    "# ROC curve for boosting ensemble using full classification trees\n",
    "bst_probs=bstlfy.predict_proba(wine_test.loc[:,['price','regn_enc', \\\n",
    "    'var_enc','wnry_enc']])\n",
    "fpr3,tpr3,thr3=roc_curve(np.where(wine_test['point_bins']=='90+',1.,0.), \\\n",
    "    bst_probs[:,0])\n",
    "\n",
    "# Plot ROC Curves\n",
    "plt.plot(fpr1,tpr1,color='#4d4d33',label='Baseline CART')\n",
    "plt.plot(fpr2,tpr2,color='#0080ff',label='Bagging Ensemble')\n",
    "plt.plot(fpr3,tpr3,color='#ff3300',label='Boosting Ensemble')\n",
    "plt.plot([0.,1.],[0.,1.],color='k',linestyle='--')\n",
    "plt.title('ROC Curves for 90+ Point Wine Classification')\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.legend(fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Models/decisionTree_best_model.pickle', 'wb') as output:\n",
    "    pickle.dump(best_lrc, output)\n",
    "    \n",
    "with open('Models/decisionTree_best_model_details.pickle', 'wb') as output:\n",
    "    pickle.dump(df_models_lrc, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

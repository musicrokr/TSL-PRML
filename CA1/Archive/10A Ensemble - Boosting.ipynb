{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processed Data Frame [ Do Not Use this, Use the Training, Tesing and Validation Data Sets Instead]\n",
    "data_path = \"./Pickles/all_articles_processed.pickle\"\n",
    "with open(data_path, 'rb') as data:\n",
    "    all_articles = pickle.load(data)\n",
    "    \n",
    "    \n",
    "#TD-IDF Features    \n",
    "#Training Features\n",
    "training_features_path = \"./Pickles/tdidf_training_features.pickle\"\n",
    "with open(training_features_path, 'rb') as data:\n",
    "    tdidf_training_features = pickle.load(data)\n",
    "    \n",
    "#Training Labels\n",
    "training_labels_path = \"./Pickles/tdidf_training_labels.pickle\"\n",
    "with open(training_labels_path, 'rb') as data:\n",
    "    tdidf_training_labels = pickle.load(data)\n",
    "    \n",
    "#Test Features\n",
    "test_features_path = \"./Pickles/tdidf_test_features.pickle\"\n",
    "with open(test_features_path, 'rb') as data:\n",
    "    tdidf_test_features = pickle.load(data)\n",
    "    \n",
    "#Test Labels\n",
    "test_labels_path = \"./Pickles/tdidf_test_labels.pickle\"\n",
    "with open(test_labels_path, 'rb') as data:\n",
    "    tdidf_test_labels = pickle.load(data)\n",
    "    \n",
    "#Validation Features\n",
    "test_features_path = \"./Pickles/tdidf_validation_features.pickle\"\n",
    "with open(test_features_path, 'rb') as data:\n",
    "    tdidf_validation_features = pickle.load(data)\n",
    "    \n",
    "#Validation Labels\n",
    "test_labels_path = \"./Pickles/tdidf_validation_labels.pickle\"\n",
    "with open(test_labels_path, 'rb') as data:\n",
    "    tdidf_validation_labels = pickle.load(data)\n",
    "    \n",
    "    \n",
    "#Sequence Vector Features    \n",
    "#Training Features\n",
    "training_features_path = \"./Pickles/sv_training_features.pickle\"\n",
    "with open(training_features_path, 'rb') as data:\n",
    "    sv_training_features = pickle.load(data)\n",
    "    \n",
    "#Training Labels\n",
    "training_labels_path = \"./Pickles/sv_training_labels.pickle\"\n",
    "with open(training_labels_path, 'rb') as data:\n",
    "    sv_training_labels = pickle.load(data)\n",
    "    \n",
    "#Test Features\n",
    "test_features_path = \"./Pickles/sv_test_features.pickle\"\n",
    "with open(test_features_path, 'rb') as data:\n",
    "    sv_test_features = pickle.load(data)\n",
    "    \n",
    "#Test Labels\n",
    "test_labels_path = \"./Pickles/sv_test_labels.pickle\"\n",
    "with open(test_labels_path, 'rb') as data:\n",
    "    sv_test_labels = pickle.load(data)\n",
    "    \n",
    "#Validation Features\n",
    "test_features_path = \"./Pickles/sv_validation_features.pickle\"\n",
    "with open(test_features_path, 'rb') as data:\n",
    "    sv_validation_features = pickle.load(data)\n",
    "    \n",
    "#Validation Labels\n",
    "test_labels_path = \"./Pickles/sv_validation_labels.pickle\"\n",
    "with open(test_labels_path, 'rb') as data:\n",
    "    sv_validation_labels = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation for Hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'deviance', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'presort': 'auto', 'random_state': 8, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "gb_0 = GradientBoostingClassifier(random_state = 8)\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "print(gb_0.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 800], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 40, None], 'min_samples_split': [10, 30, 50], 'min_samples_leaf': [1, 2, 4], 'learning_rate': [0.1, 0.5], 'subsample': [0.5, 1.0]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# n_estimators\n",
    "n_estimators = [200, 800]\n",
    "\n",
    "# max_features\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# max_depth\n",
    "max_depth = [10, 40]\n",
    "max_depth.append(None)\n",
    "\n",
    "# min_samples_split\n",
    "min_samples_split = [10, 30, 50]\n",
    "\n",
    "# min_samples_leaf\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# learning rate\n",
    "learning_rate = [.1, .5]\n",
    "\n",
    "# subsample\n",
    "subsample = [.5, 1.]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'learning_rate': learning_rate,\n",
    "               'subsample': subsample}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed: 27.9min\n"
     ]
    }
   ],
   "source": [
    "# First create the base model to tune\n",
    "gbc = GradientBoostingClassifier(random_state=8)\n",
    "\n",
    "# Definition of the random search\n",
    "random_search = RandomizedSearchCV(estimator=gbc,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=50,\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   random_state=8,n_jobs=10)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(tdidf_training_features, tdidf_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best hyperparameters from Random Search are:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "max_depth = [5, 10, 15]\n",
    "max_features = ['sqrt']\n",
    "min_samples_leaf = [2]\n",
    "min_samples_split = [50, 100]\n",
    "n_estimators = [800]\n",
    "learning_rate = [.1, .5]\n",
    "subsample = [1.]\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': max_depth,\n",
    "    'max_features': max_features,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'n_estimators': n_estimators,\n",
    "    'learning_rate': learning_rate,\n",
    "    'subsample': subsample\n",
    "\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "gbc = GradientBoostingClassifier(random_state=8)\n",
    "\n",
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "cv_sets = ShuffleSplit(n_splits = 3, test_size = .33, random_state = 8)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=gbc, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best hyperparameters from Grid Search are:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gbc = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gbc.fit(features_train, labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_pred = best_gbc.predict(features_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "print(\"The training accuracy is: \")\n",
    "print(accuracy_score(labels_train, best_gbc.predict(features_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "print(\"The test accuracy is: \")\n",
    "print(accuracy_score(labels_test, gbc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"Classification report\")\n",
    "print(classification_report(labels_test,gbc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_df = df[['Category', 'Category_Code']].drop_duplicates().sort_values('Category_Code')\n",
    "conf_matrix = confusion_matrix(labels_test, gbc_pred)\n",
    "plt.figure(figsize=(12.8,6))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            xticklabels=aux_df['Category'].values, \n",
    "            yticklabels=aux_df['Category'].values,\n",
    "            cmap=\"Blues\")\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('Actual')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = GradientBoostingClassifier(random_state = 8)\n",
    "base_model.fit(features_train, labels_train)\n",
    "accuracy_score(labels_test, base_model.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gbc.fit(features_train, labels_train)\n",
    "accuracy_score(labels_test, best_gbc.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tdidf_test_features.shape)\n",
    "print(tdidf_training_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classification tree                \n",
    "clf=DecisionTreeClassifier(max_features=None)\n",
    "clf=clf.fit(tdidf_training_features,tdidf_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report classification results.  training dataset first, then test.  \n",
    "train_error=tdidf_training_labels==clf.predict(tdidf_training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error=tdidf_test_labels==clf.predict(tdidf_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('   training accuracy: ','{:.1%}'.format(sum(train_error)/len(train_error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('   test accuracy: ','{:.1%}'.format(sum(test_error)/len(test_error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the number of n_estimators in ensemble functions\n",
    "max_n_ests=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to record results of ensembles.\n",
    "results=pd.DataFrame([],columns=list(['type','n_leaf','n_est', \\\n",
    "    'train_acc','test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train boosting ensemble on iterations of n_estimators=i\n",
    "# and iterations of stump max_leaf_nodes=j\n",
    "for j in [500,2000,8000,99999]:\n",
    "    clf_stump=DecisionTreeClassifier(max_features=None,max_leaf_nodes=j)\n",
    "    for i in np.arange(1,max_n_ests):\n",
    "        print(i)\n",
    "        bstlfy=AdaBoostClassifier(base_estimator=clf_stump,n_estimators=i)\n",
    "        bstlfy=bstlfy.fit(tdidf_training_features,tdidf_training_labels)\n",
    "        bst_tr_err=tdidf_training_labels==bstlfy.predict(tdidf_training_features)\n",
    "        bst_tst_err=tdidf_test_labels==clf.predict(tdidf_test_features)\n",
    "        run_rslt=pd.DataFrame([['bst',j,i,sum(bst_tr_err)/len(bst_tr_err),\n",
    "            sum(bst_tst_err)/len(bst_tst_err)]],\n",
    "            columns=list(['type','n_leaf','n_est','train_acc','test_acc']))\n",
    "        results=results.append(run_rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Boosting accuracy results on test data\n",
    "# 500 leaf stumps\n",
    "plt.plot(results.loc[((results.type=='bst')&(results.n_leaf==500)), \\\n",
    "    ['n_est']],results.loc[((results.type=='bst')&(results.n_leaf==500)), \\\n",
    "    ['test_acc']],linestyle='--',linewidth=2,color='#ff704d', \\\n",
    "    label='Boosting w/ 500 leaf stump')\n",
    "# 2000 leaf stumps\n",
    "plt.plot(results.loc[((results.type=='bst')&(results.n_leaf==2000)), \\\n",
    "    ['n_est']],results.loc[((results.type=='bst')&(results.n_leaf==2000)), \\\n",
    "    ['test_acc']],linestyle='--',linewidth=2,color='#ff3300', \\\n",
    "    label='Boosting w/ 2000 leaf stump')\n",
    "# 8000 leaf stumps\n",
    "plt.plot(results.loc[((results.type=='bst')&(results.n_leaf==8000)), \\\n",
    "    ['n_est']],results.loc[((results.type=='bst')&(results.n_leaf==8000)), \\\n",
    "    ['test_acc']],linestyle='--',linewidth=2,color='#b32400', \\\n",
    "    label='Boosting w/ 8000 leaf stump')\n",
    "# Full Classification Trees (no early termination)\n",
    "plt.plot(results.loc[((results.type=='bst')&(results.n_leaf==99999)), \\\n",
    "    ['n_est']],results.loc[((results.type=='bst')&(results.n_leaf==99999)), \\\n",
    "    ['test_acc']],linestyle='--',linewidth=2,color='#661400', \\\n",
    "    label='Boosting w/ full tree')\n",
    "# Plot test accuracy of baseline classification tree\n",
    "plt.plot([1,max_n_ests],[clf_test_acc,clf_test_acc],color='k', \\\n",
    "    label='Baseline classification tree')\n",
    "plt.legend(fontsize=8)\n",
    "plt.title('Boosting Test Sample Accuracy on n_estimators')\n",
    "plt.ylim([results.loc[results.type=='bst',['test_acc']].values.min()-0.01, \\\n",
    "    results.loc[results.type=='bst',['test_acc']].values.max()+0.01])\n",
    "plt.ylabel('Test Accuracy%')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Models/decisionTree_best_model.pickle', 'wb') as output:\n",
    "    pickle.dump(best_lrc, output)\n",
    "    \n",
    "with open('Models/decisionTree_best_model_details.pickle', 'wb') as output:\n",
    "    pickle.dump(df_models_lrc, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
